{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1：计算编辑距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 动态规划计算编辑距离0\n",
    "def edit_distance(str1,str2):\n",
    "    m,n = len(str1),len(str2)\n",
    "    str1 = '0'+str1\n",
    "    str2 = '0'+str2\n",
    "    dp = [[0 for x in range(n+1)]for x in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        for j in range(n+1):\n",
    "            if min(m,n) == 0:\n",
    "                dp[m][n] = max(m,n)\n",
    "            if str1[i] == str2[j]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "            dp[i][j] = min(1+dp[i-1][j],1+dp[i][j-1],d+dp[i-1][j-1])\n",
    "    print(dp[m][n])\n",
    "    return dp[m][n]\n",
    "distanse = edit_distance('applea','applee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 动态规划计算编辑距离1\n",
    "def edit_distance1(str1,str2):\n",
    "    m,n = len(str1),len(str2)\n",
    "    dp = [[0 for x in range (n+1)]for x in range(m+1)]\n",
    "    for i in range (m+1):\n",
    "        for j in range (n+1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            elif str1[i-1] == str2[j-1]: \n",
    "                dp[i][j] = dp[i-1][j-1] \n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1],        \n",
    "                                   dp[i-1][j],         \n",
    "                                   dp[i-1][j-1])     \n",
    "  \n",
    "    return dp[m][n] \n",
    "a = edit_distance1('kapplae','applae')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2：生成指定编辑距离的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "def generate_one_dis_word(str1):\n",
    "    characters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splitwords = [(str1[:i],str1[i:])for i in range(len(str1)+1)]\n",
    "    inserts = [L+c+R for L,R in splitwords for c in characters]\n",
    "    delets = [L+R[1:] for L,R in splitwords if R]\n",
    "    removes = [L+c+R[1:] for L,R in splitwords for c in characters]\n",
    "    \n",
    "    return set(inserts+delets+removes)\n",
    "\n",
    "str1 = 'apple'\n",
    "new_list = generate_one_dis_word(str1)\n",
    "print(len(new_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86524\n"
     ]
    }
   ],
   "source": [
    "def generate_two_dis_word(str1):\n",
    "    \n",
    "    return [e2 for e1 in generate_one_dis_word(str1) for e2 in generate_one_dis_word(e1)]\n",
    "print (len(generate_two_dis_word(\"apple\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于中文分词工具，jieba 的分词\n",
    "结巴是常用的中文分词工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.529 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果：贪心/学院/一直/以来/都/专注/于/人工智能/教育\n",
      "分词结果：贪心学院/一直/以来/都/专注/于/人工智能/教育\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut('贪心学院一直以来都专注于人工智能教育',cut_all = False)\n",
    "print('分词结果：' + \"/\".join(seg_list))\n",
    "\n",
    "jieba.add_word(\"贪心学院\")\n",
    "seg_list = jieba.cut('贪心学院一直以来都专注于人工智能教育', cut_all=False)\n",
    "print('分词结果：' + \"/\".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断能否把字符串拆分为字典中的词语组合\n",
    "动态规划"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = [\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\"]\n",
    "def word_break(str1):\n",
    "    dp = [False]*(len(str1)+1)\n",
    "    dp[0] = True\n",
    "    for i in range(1,len(dp)):\n",
    "        for j in range(0,i):\n",
    "            if str1[j:i] in word_dict and dp[j] == True:\n",
    "                dp[i] = True\n",
    "                \n",
    "    return dp[len(str1)] == True\n",
    "\n",
    "\n",
    "assert word_break(\"贪心科技在线教育\")==True\n",
    "assert word_break(\"在线教育是\")==False\n",
    "assert word_break(\"\")==True\n",
    "assert word_break(\"在线教育人工智能\")==True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 给定一个词典和字符串，返回所有可能有效分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['贪心', '科技', '专注于', '人工智能', '在线', '教育'], ['贪心科技', '专注于', '人工智能', '在线', '教育']]\n"
     ]
    }
   ],
   "source": [
    "word_dict = set([\"贪心科技\", \"人工智能\", \"教育\", \"在线\", \"专注于\",\"贪心\",\"科技\"])\n",
    "max_len_word = 5\n",
    "def all_possible_segmentations(str1):\n",
    "    segs = []\n",
    "    if len(str1) == 0:\n",
    "        return segs\n",
    "    max_splits = min(len(str1),max_len_word)+1\n",
    "    for i in range (1,max_splits):\n",
    "        word = str1[0:i]\n",
    "        if word in word_dict:\n",
    "            sub_str = all_possible_segmentations(str1[i:])\n",
    "            if sub_str == [] and len(sub_str) == 0:\n",
    "                segs.append([word])\n",
    "            else:\n",
    "                for seg in sub_str:\n",
    "                    seg = [word]+seg\n",
    "                    segs.append(seg)\n",
    "    return segs\n",
    "\n",
    "a = all_possible_segmentations('贪心科技专注于人工智能在线教育')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停用词过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please input your words:a an thr the dog pig bird\n",
      "word_all: ['a', 'an', 'thr', 'the', 'dog', 'pig', 'bird']\n",
      "filter_words: ['thr', 'dog', 'pig', 'bird']\n"
     ]
    }
   ],
   "source": [
    "# 1：自己建立停用词词库\n",
    "stop_words = ['a','an','the','am','is','are']\n",
    "shuru = input('please input your words:').strip()\n",
    "word_all = shuru.split(' ')\n",
    "print('word_all:',word_all)\n",
    "filter_words = [word for word in word_all if word not in stop_words]\n",
    "print('filter_words:',filter_words)\n",
    "\n",
    "# 2：使用现成的停用词库\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "test_strs = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "         'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "         'meeting', 'stating', 'siezing', 'itemization',\n",
    "         'sensational', 'traditional', 'reference', 'colonizer',\n",
    "         'plotted']\n",
    "\n",
    "singles = [stemmer.stem(word) for word in test_strs]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋向量： 把文本转换成向量 , 只有向量才能作为模型的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 1 0 0 2 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 2 0 0 2 0 1]]\n",
      "['actually', 'and', 'beijing', 'but', 'car', 'denied', 'from', 'going', 'he', 'in', 'is', 'lied', 'lost', 'mike', 'my', 'phone', 'request', 'shanghai', 'the', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "# 方法1： 词袋模型（按照词语出现的个数）\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "     'He is going from Beijing to Shanghai.',\n",
    "     'He denied my request, but he actually lied.',\n",
    "     'Mike lost the phone, and phone was in the car.',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.39379499 0.         0.         0.\n",
      "  0.39379499 0.39379499 0.26372909 0.         0.39379499 0.\n",
      "  0.         0.         0.         0.         0.         0.39379499\n",
      "  0.         0.39379499 0.        ]\n",
      " [0.35819397 0.         0.         0.35819397 0.         0.35819397\n",
      "  0.         0.         0.47977335 0.         0.         0.35819397\n",
      "  0.         0.         0.35819397 0.         0.35819397 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.26726124 0.         0.         0.26726124 0.\n",
      "  0.         0.         0.         0.26726124 0.         0.\n",
      "  0.26726124 0.26726124 0.         0.53452248 0.         0.\n",
      "  0.53452248 0.         0.26726124]]\n",
      "['actually', 'and', 'beijing', 'but', 'car', 'denied', 'from', 'going', 'he', 'in', 'is', 'lied', 'lost', 'mike', 'my', 'phone', 'request', 'shanghai', 'the', 'to', 'was']\n"
     ]
    }
   ],
   "source": [
    "# 方法2：词袋模型（tf-idf方法）\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print (X.toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
